
---
title: "Final Project-College Majors Data Set"
author: "Kashyapee Mokal"
date: "06/12/2021"
output:
  html_document: default
  word_document: default
---

1:Introduction:- The data set that interested me the most is mainly about the analysis of earnings with respect to college majors. 
My Research question on this data set is-What influence do different factors, while choosing a major have, on a student's career?The reason it interests me is, it provides  a holistic view on different aspects for choosing a major and while answering some important questions,it helps  making an informed decision for the future. 
Some of these questions are-
1:Which Major Category provides high salary jobs?
2:Which Major Category has highest Unemployment rate and which has highest employment rate?
3: Do women prefer high salaried jobs or low salaried jobs?
4:Is there a gender bias in choosing a major related to STEM(Engineering)?
5:Does employment require college degree jobs?
All these are important to know to make a wise decision while choosing a career and making better life decisions.Each individual must be interested in which career option they must opt keeping in mind all of these factors.

2:Data:-The data set I am choosing for my final project is recent grads data set from a folder of college majors, available on FiveThirtyEight(https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/).
This folder  includes many sub data sets, while the one that provided a proper insight and a detailed breakdown of many different factors was recent grads data set.
All data is collected from American Community Survey 2010-2012 Public Use Microdata Series, which is an experimental study.
Unit of Observation- Each unit of observation or row represents a Major.There 173 rows and 21 columns(variables).
Variables I am interested in studying are-Major, Major Category, Total,Men and Women,Median,ShareWomen, Employed and College Jobs.
Variables in the data set represent:
Major=Major description,Major_category= Category of Major.
Total= Total number of people with Major, Men=Male graduates, Women=Female graduates, Median=Median earnings of full-time, year-round workers.
ShareWomen=Women as share of total,Employed=Number employed.
College Jobs=Number of jobs that require a college degree.
Summary of the dataset-
```{r}
library(tidyverse)
CM= (read.csv("recent-grads.csv"))
head(CM)
summary(CM)
```
Data Cleaning And Transformation:
Here we don't really need the major code variable for our analysis, so we can remove it and shorten the table. Also need to remove all the NA's in the table.
```{r}
CM1 = select(CM,-Major_code)
CM2=na.omit(CM1) 
```
Normalizing the data set for better analysis:
```{r}
numdata=CM2[,c(1,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,6,2)]
normalize_data=function(x){
  return((x-min(x))/(max(x)-min(x)))
  }
Major_category=numdata$Major_category
Major=numdata$Major
numdata_n=cbind(as.data.frame(lapply(numdata[,1:18], normalize_data)),Major_category,Major)
```

3:Exploratory Data Analysis:-
Let's perform exploratory data analysis for some of the questions which can provide a better insight for the data.
1:Which Major Category provides high salary jobs?
```{r}
CM5= numdata_n %>% group_by(Major_category) %>% summarise(Major_category,Median,Major) %>% arrange(desc(Median))
head(CM5)
```
We can observe from the table above that Major Petroleum Engineering falling in the major category of engineering has the highest median salary followed by Mining and Mineral Energy Engineering.
Let's visualize and check if Engineering has the highest paid salaries.
```{r}
ggplot(CM5,aes(x=Major_category, y=Median,fill=Major_category))+ geom_col()+coord_flip()+ggtitle('Comparison of median salaries with respect to Major Categories')
```
As we can see from the above bar plot Engineering has the highest paid jobs whereas interdisciplinary has lowest paid jobs.

2:Which Major Category has highest Unemployment rate and which has highest Employment rate?
```{r}
CM7= numdata_n %>% group_by(Major_category) %>% mutate(Employment_Rate=Employed/(sum(Employed+Unemployed))) %>% summarise(Employment_Rate,Unemployment_rate) %>% arrange(desc(Employment_Rate))
head(CM7)
ggplot(CM7,aes(y=Employment_Rate,x=Unemployment_rate,col=Major_category))+geom_point()
```
We can see from the above table and plot that Interdisciplinary major category has the highest employment rate whereas Engineering has highest unemployment rate.  

3: Do women prefer high salaried jobs or low salaried jobs?
```{r}
zx=CM2 %>% group_by(Major_category) %>% summarise(Median,ShareWomen,Major)%>% arrange(desc(Median))
head(zx)
ggplot(zx,aes(x=ShareWomen,y=Median, col=Major_category))+geom_point()+geom_text(mapping = aes(label=abbreviate(Major_category)),cex=2.5,hjust=0,vjust=2)
```
As we can see from the above point graph that most women have low salaried jobs. The outlier is for Engineering major category which is Petroleum Engineering major having the highest salary and lowest is for Education.We can also observe that most high paying jobs are from Engineering major category.

4:Is there a gender bias in choosing a major related to STEM(Engineering)?
As Engineering is considered to be a male dominated field is it true based on the data that men are more in Engineering as compared to women?
```{r}
CM6= CM2 %>% filter(Major_category=='Engineering') %>% group_by(Major) %>% summarise(Women, Men) %>% arrange(desc(Women,Men)) %>% gather(key = "Gender",value = "Enrolled",c(Women,Men))
boxplot(CM6$Enrolled~CM6$Gender,col=c("green","red"),xlab = "Gender",ylab = "Total",main="Gender Comparison w.r.t to engineering category")
```
It is evident from the boxplot that Engineering in this data seems to be dominated by men. Women are much less in comparison to that of Men.

4:Data Analysis:
The two techniques i used for data analysis are Linear Regression and PCA.

Linear Regression:
5:Does employment require college degree jobs?
As not all people get a job right after their degree is it worth taking a degree? Does it ensure jobs after college or would it be better to take non-college jobs avoiding the hassle of paying heavy tuition fees.
```{r}
mkn=select(numdata_n,c('Employed','College_jobs','Non_college_jobs','Low_wage_jobs'))
library(corrplot)
cor(mkn)
corrplot(cor(mkn),method = "circle")
```
We can see the correlation value between Employed and college jobs is 0.797 or round it up to 80%, between Employed and Non-college jobs is 94%. Also the correlation between College jobs and low wages is 65% and Non-college jobs is 98% which is very close to 1 ,indicating strong relationship.
So since Non college jobs and Employed variables show a strong relationship, we can fit a linear regression model for it.

Fitting a linear regression model where Employment rate is the dependent variable and Non College jobs is the predictor variable.
```{r}
yz=numdata_n %>% group_by(Major_category) %>% mutate(Employment_rate=Employed/sum(Employed+Unemployed)) %>% mutate(Non_college_jobs_rate=Non_college_jobs/sum(College_jobs+Non_college_jobs)) %>% summarise(Employment_rate,Non_college_jobs_rate,Major) %>% arrange(desc(Employment_rate))
ggplot(yz,aes(x=Non_college_jobs_rate,y=Employment_rate, col=Major_category))+geom_point()
```
From the above table and graph we can observe that highest number of employed people with non college jobs are from Interdisciplinary major category followed by law and public policy.

Dividing the data set into training and testing data set and fitting a linear model on the training data set:-
```{r}
library(caTools)
set.seed(2)
split_data=sample.split(yz,SplitRatio=0.7)
split_data
train_data= subset(yz,split_data="TRUE")
train_data
test_data= subset(yz,split_data="FALSE")
test_data
regression=lm(Employment_rate~Non_college_jobs_rate,data=train_data)
plot(Employment_rate~Non_college_jobs_rate,yz)
abline(regression)
summary(regression)
predict= predict(regression,test_data)
```
From the plot we can see there is a positive linear relationship between Non college jobs and Employment rate variables.
Let's understand the summary:
Residuals is the difference between the observed and the standard value. The median is close to 0 and the min, max and 1st Quartile and 3rd Quartile have magnitude close to each other so we can say the model is symmetric.
With every increase in Non college jobs rate the Employment rate increases by 0.83. Standard error is also small.T value has magnitude more than 0 which is 28.332, so it is statistically significant.P value is  less than 2e-16 which is very small, less than 0.05 so we can say that  Non college jobs rate and Employment rate are two different parameters and the result is not random.Residual standard error is also small, because the line fits many of the observations. R square and multiple r square are not much different as there is just one predictor variable in this data set. We can see that Multiple R squared value is 0.825 so the predictor variable indicates 83% variance. F statistic is much more than 1 so it indicates a good relationship between the variables. Degrees of freedom is 170 as there are 172 observations and 2(including y intercept)variables.172-2=170. P value is much smaller than 0.05 which indicates it is statistically significant.
```{r}
cor(yz$Employment_rate,yz$Non_college_jobs_rate)
cor.test(yz$Employment_rate,yz$Non_college_jobs_rate)
```
We can see that the correlation is positive and is 0.908 which is approximately equal to 91%. Meaning, as the Non college jobs rate increases we see an increase in the Employment rate. Correlation test rejects the null hypothesis which says correlation is equal to 0, which is rejected with a p value of 2.2e-16 which is much smaller than 0.05.

Predicting:-
1:Plotting original values:
```{r}
plot(test_data$Employment_rate,type = "l",lty=1.8,col="green")
```
2:Plotting predicted values:
```{r}
plot(predict,type = "l",lty=1.8,col="red")
```
3:Plotting Predicted vs Actual Values:
```{r}
plot(test_data$Employment_rate,type = "l",lty=1.8,col="green")
lines(predict,type="l",lty=1.8,col="blue")
```
We can see that our model has made a good a prediction, most of the blue line fits well on the green line indicating that predicted values are mostly correct.

Performing Adequacy Checks for Assumptions:
1: Linearity.
2: Normal Distribution.

```{r}
library(olsrr)
ols_plot_resid_fit(regression)
ols_plot_resid_qq(regression)
```


The above residual plot show that linearity assumption has been met. But normality assumption is violated. We need to perform some transformation so the normality assumption is also met. 

Performing Sqaure root ransformation method on the regression model:
```{r}
regression1=lm(sqrt(Employment_rate)~Non_college_jobs_rate,data=yz)
residual=resid(regression1)
qqnorm(residual)
qqline(residual)
```

After the square root transformation we can see that normality assumption has also been met to most extent.

Principal Component Analysis:
We can observe that there are 17 numerical variables in the data set. We can perform dimensionality reduction using PCA technique to reduce the number of dimensions. To particularly check if Major Category Engineering and Arts form separate clusters,meaning different factors affect both the major categories.
```{r}
numbers=select(CM2,-Major,-Major_category)
vara=var(numbers)
pca=prcomp(numbers[2:18],scale. = TRUE)
summary(pca)
```

Principal components are the eigen vectors. So, the eigen vector that constitutes the most variance is the 1st eigen value which is the 1st principal component. So, the most variance is for PC1 followed by PC2 and till PC17 we can see the variance is 0. Here, in proportion of variance we can see that PC1 accounts for 64% variance, PC2 is 18%, PC3 6% and PC4 4% and so on till PC17 has 0% variance. We can see in cumulative proportion all variances up to PC4 constitute for 93% of the variance. So, we can consider 1st 4 principal components for our analysis, since they constitute much of variance. The dimensions can be reduced from 17 to 4.

Plotting PC1 and PC2:-
```{r}

numdata_n1=cbind(numbers,pca$x[,1:2])
cor(numbers,numdata_n1[19:20])
plot(pca,type='l',main="Scree plot for principal component analysis")
```
As we can see from the Scree plot, that PC1 constitutes for the main variance. However, after PC2 the variance decreases considerably. Summarizing the correlation table, we can see that all the 17 variables increase with increase in PC1 value, except for Median, P25th and P75th salary. While for PC2 we can see that variables Rank, Women, Share Women, Part_time, Unemployment rate, non_college jobs and low_wage_jobs increase with increase in PC2 while all other variables decrease with increase in PC2.
```{r}
numdata_n2= cbind(CM2,numdata_n1[19:20])  %>% filter( Major_category=='Engineering'| Major_category=='Arts') %>% group_by(Major_category)
ggplot(numdata_n2,aes(PC1,PC2,col=Major_category,fill=Major_category))+stat_ellipse(geom='polygon',col="black",alpha=0.5)+geom_point(shape=21,col="black")
```
Here from the above plot, we can see that two clusters are formed for Arts and Engineering Major Category. Even though some overlap has been observed, we can still say that different factors affect Engineering and Arts Major category. While we can see for Engineering Major Category it has high PC2 values, but still less in comparison with Arts and has low PC1 values, whereas for Arts most points are above 0, it has high PC1 and PC2 values.

5: Conclusion: Summarizing the analysis, in the data set and answering my research questions, we can see from the above first two questions that even though Engineering has high paid jobs it has highest unemployment rate while interdisciplinary being the lowest paid has highest employment rate. This analysis can leads us to the conclusion that due to high salaries most people are drawn towards Engineering but not many seek Employment after it.On the other hand Interdisciplinary has highest employment because there isn't much competition and most people land up a job even it is low salaried. 
College degrees that ensure employment are less as compared to jobs that don't require a college degree but have low salaried jobs as compared to the College Jobs.
We can observe that most women tend to prefer jobs with low salaries except for Engineering which offers a good pay.
Even though Engineering has good salary we can see it is dominated by men, and we can notice the gender bias for it.Men in Engineering are higher as compared to the women. As the number of men are much higher in Engineering we can say that men are more inclined towards high salaried job as Engineering has the highest pay.
Linear regression predicts the Employment rate with respect to the non college jobs rate. Here we can analyse that even the employment is more for jobs which don't require a college degree as compared to the ones who require a college degree. But it non college jobs have more low wages jobs as compared to College jobs.
From PCA we can say that different factors affect Engineering and Arts major category leading to the formation of clusters and we observed that 17 variables can be decomposed within 4 main principal components to account for most variability.
In conclusion we can say that many factors can influence Major categories, while the major factors are the employment, the salary,college and non college jobs and the gender.This data set provides a good insight for choosing the right direction for one's future though it doesn't involve the fees for each major which could provide better return on investment analysis.If this factor could be added in future, it would provide a holistic information for better analysis.

6: References:
https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/
https://www.rdocumentation.org/packages/VGAM/versions/1.1-5/topics/yeo.johnson
https://cran.r-project.org/web/packages/olsrr/vignettes/heteroskedasticity.html#:~:text=Introduction,problem%20is%20known%20as%20heteroscedasticity.
https://people.revoledu.com/kardi/tutorial/Similarity/Normalization.html
